{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import git\n",
    "import mlflow.keras\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.preprocessing import *\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/joshib/cs_data/citation-bio-labelled-data-2020-06-08~15:13:57.766608/nlp/exps/output/2020-06-08~15:13:57.766608'\n",
    "#data_df = pd.read_csv(path+'/data-2020-06-08~15:14:01.185827.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with random embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 1.06 s, total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train_data_path = '/home/joshib/cs_data/citation-bio-labelled-data-2020-06-09~06:30:11.273245/nlp/exps/output/2020-06-09~06:30:11.273245'\n",
    "#train_data_path = '/nlp/cs_data/2020-06-09~06:30:11.273245'\n",
    "train_data_path = '/nlp/cs_data_full'\n",
    "\n",
    "pattern = \"*#*#\"\n",
    "sentences = []\n",
    "sent_tags = []\n",
    "len_arr = []\n",
    "for fpath in os.listdir(train_data_path):\n",
    "    if fpath not in ['data-gen-config.json', 'data_generation_stats.csv'] and \".csv\" in fpath:\n",
    "        fpath = os.path.join(train_data_path, fpath)\n",
    "        df = pd.read_csv(fpath, index_col=0)\n",
    "        df.fillna(\"\\n\", axis=1, inplace=True)\n",
    "        len_arr.append(df.shape[0])\n",
    "        sentences.append(pattern.join(df.x))\n",
    "        sent_tags.append(pattern.join(df.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 26\n",
      "Max: 7272\n",
      "Mean: 1602\n",
      "Median: 1543\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(\"Min: %d\"%numpy.min(len_arr))\n",
    "print(\"Max: %d\"%numpy.max(len_arr))\n",
    "print(\"Mean: %d\"%numpy.mean(len_arr))\n",
    "print(\"Median: %d\"%numpy.median(len_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 4.77 s, total: 3min 3s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "maxlen = max(len_arr)\n",
    "X = [[w for w in s.split(pattern)] for s in sentences]\n",
    "y = [[p for p in t.split(pattern)] for t in sent_tags]\n",
    "\n",
    "new_X = []\n",
    "new_y = []\n",
    "for ind in range(len(X)):\n",
    "    new_seq = []\n",
    "    new_tag = []\n",
    "    for i in range(maxlen):\n",
    "        try:\n",
    "            new_seq.append(X[ind][i])\n",
    "            new_tag.append(y[ind][i])\n",
    "        except:\n",
    "            new_seq.append(\"PADword\")\n",
    "            new_tag.append(\"I-CIT\")\n",
    "    new_X.append(new_seq)\n",
    "    new_y.append(new_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "#maxlen = 1000 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.4 s, sys: 1.31 s, total: 46.7 s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tags = df.y.unique()\n",
    "tags2index = {'B-CIT': 1, 'I-CIT': 0}\n",
    "new_y_enc = np.array([[tags2index[w] for w in s] for s in new_y])\n",
    "#y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tags2index[\"I-CIT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(new_X, new_y_enc, test_size=0.1, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=1)\n",
    "tokenizer.fit_on_texts(list(X_train_orig))\n",
    "X_train = np.array(tokenizer.texts_to_sequences(X_train_orig))\n",
    "X_test  = np.array(tokenizer.texts_to_sequences(X_test_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with pre-trained fast text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 9s, sys: 3.94 s, total: 3min 13s\n",
      "Wall time: 3min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_model = FastText.load_fasttext_format('/nlp/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n",
      "number of null word embeddings: 10138\n",
      "CPU times: user 8.06 s, sys: 230 ms, total: 8.29 s\n",
      "Wall time: 8.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_embedding_matrix(embeddings, nb_words, word_index, embed_dim):  # load embeddings\n",
    "    embeddings_index = {}\n",
    "    for word in embeddings.wv.vocab:\n",
    "        embeddings_index[word] = embeddings[word]\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    words_not_found = []\n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = load_embedding_matrix(wiki_model, max_features, tokenizer.word_index, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('embedding_matrix.npy', embedding_matrix)\n",
    "#embedding_matrix = np.load('./embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7272, 300)         30000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 7272, 50)          70200     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 7272, 1)           51        \n",
      "=================================================================\n",
      "Total params: 30,070,251\n",
      "Trainable params: 30,070,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 749 ms, sys: 374 ms, total: 1.12 s\n",
      "Wall time: 493 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 300, weights=[embedding_matrix], trainable=True, input_length= maxlen))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation=\"sigmoid\")))\n",
    "# model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0,\n",
    "        epsilon=1e-05,\n",
    "        amsgrad=False,\n",
    "    ),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulate_ser_jer(y_true, y_pred, keep_tag):\n",
    "    \"\"\"\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param keep_tag:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_true, y_pred=y_pred).ravel()\n",
    "    ser = 1.0\n",
    "    if (tp + fp) > 0.0:\n",
    "        ser = fp / float(tp + fp)\n",
    "    jer = 1.0\n",
    "    if (tp + fn) > 0.0:\n",
    "        jer = fn / float(tp + fn)\n",
    "    return ser, jer\n",
    "\n",
    "class Metrics(callbacks.Callback):\n",
    "    def __init__(self, train_data, validation_data):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_ser = []\n",
    "        self.val_jer = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "#         if self.validation_data is None:\n",
    "#             return \n",
    "        #print(self.validation_data)\n",
    "        #print(self.model.get_weights()[0])\n",
    "        vx = self.validation_data[0]\n",
    "        vy = self.validation_data[1]\n",
    "        vx = np.array(vx)\n",
    "        pred_y = None\n",
    "        pred_y = self.model.predict(vx)\n",
    "        py = np.argmax(pred_y, axis=-1)\n",
    "        #vy = np.argmax(vy, axis=-1)\n",
    "        vy = np.array(vy).flatten()\n",
    "        py = np.array(py).flatten()\n",
    "        ser, jer = calulate_ser_jer(vy, py, 1)\n",
    "        print(ser, jer)\n",
    "        self.val_ser.append(ser)\n",
    "        self.val_jer.append(jer)\n",
    "        logs[\"val_ser\"] = ser\n",
    "        logs[\"val_jer\"] = jer\n",
    "        print(f\"— val_ser: {ser} — val_jer: {jer}\")\n",
    "\n",
    "#         gold_x, gold_y, x = df_to_input()\n",
    "#         pred_y = self.model.predict(x)\n",
    "#         py = np.argmax(pred_y, axis=-1)[0]\n",
    "#         gy = np.argmax(gold_y, axis=-1)\n",
    "#         data = []\n",
    "#         for i in range(len(gold_x)):\n",
    "#             data.append({\"x\": gold_x[i], \"gold\": gy[i], \"pred\": py[i]})\n",
    "#         d = pd.DataFrame(data)\n",
    "#         d.to_csv(\"./\" + str(epoch) + \".predictions.on.sample.csv\")\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics([X_train, y_train], [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1095/1095 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.99751.0 1.0\n",
      "— val_ser: 1.0 — val_jer: 1.0\n",
      "1095/1095 [==============================] - 715s 653ms/step - loss: 0.0090 - accuracy: 0.9975 - val_loss: 4.4780e-05 - val_accuracy: 1.0000 - val_ser: 1.0000 - val_jer: 1.0000\n",
      "Epoch 2/200\n",
      "1095/1095 [==============================] - ETA: 0s - loss: 2.3828e-05 - accuracy: 1.00001.0 1.0\n",
      "— val_ser: 1.0 — val_jer: 1.0\n",
      "1095/1095 [==============================] - 713s 651ms/step - loss: 2.3828e-05 - accuracy: 1.0000 - val_loss: 1.5238e-05 - val_accuracy: 1.0000 - val_ser: 1.0000 - val_jer: 1.0000\n",
      "Epoch 3/200\n",
      "1095/1095 [==============================] - ETA: 0s - loss: 1.0485e-05 - accuracy: 1.00001.0 1.0\n",
      "— val_ser: 1.0 — val_jer: 1.0\n",
      "1095/1095 [==============================] - 712s 650ms/step - loss: 1.0485e-05 - accuracy: 1.0000 - val_loss: 9.5215e-06 - val_accuracy: 1.0000 - val_ser: 1.0000 - val_jer: 1.0000\n",
      "Epoch 4/200\n",
      "1095/1095 [==============================] - ETA: 0s - loss: 6.8345e-06 - accuracy: 1.00001.0 1.0\n",
      "— val_ser: 1.0 — val_jer: 1.0\n",
      "1095/1095 [==============================] - 715s 653ms/step - loss: 6.8345e-06 - accuracy: 1.0000 - val_loss: 7.2786e-06 - val_accuracy: 1.0000 - val_ser: 1.0000 - val_jer: 1.0000\n",
      "Epoch 5/200\n",
      " 961/1095 [=========================>....] - ETA: 1:18 - loss: 5.2413e-06 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#early_stop= EarlyStopping(monitor='val_loss',patience=3,verbose=0,mode='min',restore_best_weights=False, min_delta=0.0001)\n",
    "history = model.fit(X_train, y_train, verbose=1, epochs=200, validation_data=(X_test, y_test), callbacks=[metrics])\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "axes[0].plot(history.history['accuracy'])\n",
    "axes[0].plot(history.history['val_accuracy'])\n",
    "axes[0].title.set_text('model accuracy')\n",
    "axes[0].set_ylabel('accuracy')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "axes[1].plot(history.history['loss'])\n",
    "axes[1].plot(history.history['val_loss'])\n",
    "axes[1].title.set_text('model loss')\n",
    "axes[1].set_ylabel('loss')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].legend(['train', 'val'], loc='upper left')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras with random embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2223, 300)         30000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2223, 1024)        3330048   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 2223, 1)           1025      \n",
      "=================================================================\n",
      "Total params: 33,331,073\n",
      "Trainable params: 33,331,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 1.82 s, sys: 309 ms, total: 2.13 s\n",
      "Wall time: 985 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
    "#model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(1, activation=\"sigmoid\")))\n",
    "# # model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.005,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0,\n",
    "        epsilon=1e-05,\n",
    "        amsgrad=False,\n",
    "    ),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 45 samples\n",
      "Epoch 1/5\n",
      "400/400 [==============================] - 21s 53ms/sample - loss: 0.2213 - accuracy: 0.9592 - val_loss: 0.1568 - val_accuracy: 0.9803\n",
      "Epoch 2/5\n",
      "400/400 [==============================] - 17s 43ms/sample - loss: 0.1173 - accuracy: 0.9802 - val_loss: 0.0976 - val_accuracy: 0.9803\n",
      "Epoch 3/5\n",
      "400/400 [==============================] - 17s 43ms/sample - loss: 0.1049 - accuracy: 0.9802 - val_loss: 0.1092 - val_accuracy: 0.9803\n",
      "Epoch 4/5\n",
      "400/400 [==============================] - 17s 43ms/sample - loss: 0.1015 - accuracy: 0.9802 - val_loss: 0.0960 - val_accuracy: 0.9803\n",
      "Epoch 5/5\n",
      "400/400 [==============================] - 17s 43ms/sample - loss: 0.0984 - accuracy: 0.9805 - val_loss: 0.1015 - val_accuracy: 0.9803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f679a8e0550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, verbose=1, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import tensorflow as tf\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(1000, size=(10000, 80))\n",
    "y = np.random.choice([0,1], size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 80, 50)            50000     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                2440      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 52,451\n",
      "Trainable params: 52,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 50, input_length = 80))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print ('Compiling...')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": " [_Derived_]  Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/StatefulPartitionedCall]]\n\t [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_30]] [Op:__inference_train_function_3031]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4c1a2828403e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  [_Derived_]  Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/lstm/StatefulPartitionedCall]]\n\t [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_30]] [Op:__inference_train_function_3031]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oth_df = pd.read_csv(\"/nlp/other_ref_citations_from_64k_data_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pii</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>S0272884218334801.csv</td>\n",
       "      <td>V.Y. Osipov, A.M. Panich, A.V. Baranov, Commen...</td>\n",
       "      <td>/*/ce:bibliography/ce:bibliography-sec/ce:bib-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>S0144861718308208.csv</td>\n",
       "      <td>100 of the World's Worst Invasive Alien Specie...</td>\n",
       "      <td>/*/ce:bibliography/ce:bibliography-sec/ce:bib-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>S0144861718308208.csv</td>\n",
       "      <td>Plastics - the Facts 2015. (2015). http://www....</td>\n",
       "      <td>/*/ce:bibliography/ce:bibliography-sec/ce:bib-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>S0959652618324478.csv</td>\n",
       "      <td>Adiansyah, J.S., Haque, N., Rosano, M., Biswas...</td>\n",
       "      <td>/*/ce:bibliography/ce:bibliography-sec/ce:bib-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>S0959652618324478.csv</td>\n",
       "      <td>Ashley, S.F., Fenner, R.A., Nuttall, W.J., Par...</td>\n",
       "      <td>/*/ce:bibliography/ce:bibliography-sec/ce:bib-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                    pii  \\\n",
       "0           0  S0272884218334801.csv   \n",
       "1           1  S0144861718308208.csv   \n",
       "2           2  S0144861718308208.csv   \n",
       "3           3  S0959652618324478.csv   \n",
       "4           4  S0959652618324478.csv   \n",
       "\n",
       "                                                text  \\\n",
       "0  V.Y. Osipov, A.M. Panich, A.V. Baranov, Commen...   \n",
       "1  100 of the World's Worst Invasive Alien Specie...   \n",
       "2  Plastics - the Facts 2015. (2015). http://www....   \n",
       "3  Adiansyah, J.S., Haque, N., Rosano, M., Biswas...   \n",
       "4  Ashley, S.F., Fenner, R.A., Nuttall, W.J., Par...   \n",
       "\n",
       "                                               label  \n",
       "0  /*/ce:bibliography/ce:bibliography-sec/ce:bib-...  \n",
       "1  /*/ce:bibliography/ce:bibliography-sec/ce:bib-...  \n",
       "2  /*/ce:bibliography/ce:bibliography-sec/ce:bib-...  \n",
       "3  /*/ce:bibliography/ce:bibliography-sec/ce:bib-...  \n",
       "4  /*/ce:bibliography/ce:bibliography-sec/ce:bib-...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V.Y. Osipov, A.M. Panich, A.V. Baranov, Comment on “Carbon structure in nanodiamonds elucidated from Raman spectroscopy” by V.I. Korepanov et al., Carbon 127 (2018) 193-194. ',\n",
       " \"100 of the World's Worst Invasive Alien Species. (2013). http://www.iucngisd.org/gisd/100_worst.php. \",\n",
       " 'Plastics - the Facts 2015. (2015). http://www.plasticseurope.org/Document/plastics---the-facts-2015.aspx. ',\n",
       " 'Adiansyah, J.S., Haque, N., Rosano, M., Biswas, W., 2017. Application of a life cycle assessment to compare environmental performance in coal mine tailings management. J. Environ. Manage. 199, 181–191. https://doi.org/10.1016/j.jenvman.2017.05.050 ',\n",
       " 'Ashley, S.F., Fenner, R.A., Nuttall, W.J., Parks, G.T., 2015. Life-cycle impacts from novel thorium-uranium-fuelled nuclear energy systems. Energy Convers. Manag. 101, 136–150. https://doi.org/10.1016/j.enconman.2015.04.041 ',\n",
       " 'Awuah-Offei, K., Adekpedjou, A., 2011. Application of life cycle assessment in the mining industry. Int. J. Life Cycle Assess. 16, 82–89. https://doi.org/10.1007/s11367-010-0246-6 ',\n",
       " 'Drielsma, J.A., Russell-Vaccari, A.J., Drnek, T., Brady, T., Weihed, P., Mistry, M., Simbor, L.P., 2016. Mineral resources in life cycle impact assessment—defining the path forward. Int. J. Life Cycle Assess. 21, 85–105. https://doi.org/10.1007/s11367-015-0991-7 ',\n",
       " 'Durucan, S., Korre, A., Munoz-Melendez, G., 2006. Mining life cycle modelling: a cradle-to-gate approach to environmental management in the minerals industry. J. Clean. Prod. 14, 1057–1070. https://doi.org/10.1016/j.jclepro.2004.12.021 ',\n",
       " 'Farjana, S.H., Huda, N., Mahmud, M.A.P., Lang, C., 2018b. Towards sustainable TiO 2 production\\u202f: An investigation of environmental impacts of ilmenite and rutile processing routes in Australia. J. Clean. Prod. 196, 1016–1025. https://doi.org/10.1016/j.jclepro.2018.06.156 ',\n",
       " 'Farjana, S.H., Huda, N., Mahmud, M.A.P., Saidur, R., 2018c. Solar process heat in industrial systems – A global review. Renew. Sustain. Energy Rev. 82, 2270–2286. https://doi.org/https://doi.org/10.1016/j.rser.2017.08.065 ',\n",
       " 'Haque, N., Norgate, T., 2014. The greenhouse gas footprint of in-situ leaching of uranium, gold and copper in Australia. J. Clean. Prod. 84, 382–390. https://doi.org/10.1016/j.jclepro.2013.09.033 ',\n",
       " 'Hisan, S., Huda, N., Mahmud, M.A.P., Saidur, R., 2018. Solar industrial process heating systems in operation – Current SHIP plants and future prospects in Australia. Renew. Sustain. Energy Rev. 91, 409–419. https://doi.org/10.1016/j.rser.2018.03.105 ',\n",
       " 'Koltun, P., 2014. The impact of uranium ore grade on the greenhouse gas footprint of nuclear power. J. Clean. Prod. 84, 360–367. https://doi.org/10.1016/j.jclepro.2013.11.034 ',\n",
       " 'Lenzen, M., 2008. Life-Cycle Energy Balance and Greenhouse Gas Emissions of Nuclear Energy: A review. Energy Convers. Manag. 49, 2178–2199. https://doi.org/10.1016/j.enconman.2008.01.033 ',\n",
       " 'Long, A.E., 2011. Life Cycle Assessment Analysis Of Coal Versus Nuclear Power In Levy County, Florida 1–114. ',\n",
       " 'Mckay, A.D., Miezitis, Y., 2011. Australasian Pancreatic Club (APC) and Sydney Upper Gastrointestinal Surgical Society (SUGSS) Joint Meeting. October 15-16, 2011, Bondi, Sydney, Australia. Pancreatology 11, 567–572. https://doi.org/10.1159/000335161 ',\n",
       " 'Moran, C.J., Lodhia, S., Kunz, N.C., Huisingh, D., 2014. Sustainability in mining, minerals and energy: New processes, pathways and human interactions for a cautiously optimistic future. J. Clean. Prod. 84, 1–15. https://doi.org/10.1016/j.jclepro.2014.09.016 ',\n",
       " 'Mudd, G.M., 2008. Radon releases from Australian uranium mining and milling projects: assessing the UNSCEAR approach. J. Environ. Radioact. 99, 288–315. https://doi.org/10.1016/j.jenvrad.2007.08.001 ',\n",
       " 'Mudd, G.M., 2007. Gold mining in Australia: linking historical trends and environmental and resource sustainability. Environ. Sci. Policy 10, 629–644. https://doi.org/10.1016/j.envsci.2007.04.006 ',\n",
       " 'Mudd, G.M., Diesendorf, M., 2010. Uranium Mining, Nuclear Power and Sustainability: Rhetoric versus Reality. Sustain. Min. Conf. 2010 39. ',\n",
       " 'Mudd, G.M., Diesendorf, M., 2008. Sustainability of uranium mining and milling: Toward quantifying resources and eco-efficiency. Environ. Sci. Technol. 42, 2624–2630. https://doi.org/10.1021/es702249v ',\n",
       " 'Mutchek, M., Cooney, G., Pickenpaugh, G., Marriott, J., Skone, T., 2016. Understanding the Contribution of Mining and Transportation to the Total Life Cycle Impacts of Coal Exported from the United States. Energies 9, 559. https://doi.org/10.3390/en9070559 ',\n",
       " 'Norgate, T., Haque, N., Koltun, P., 2014. The impact of uranium ore grade on the greenhouse gas footprint of nuclear power. J. Clean. Prod. 84, 360–367. https://doi.org/10.1016/j.jclepro.2013.11.034 ',\n",
       " 'Northey, S., Haque, N., Mudd, G., 2013. Using sustainability reporting to assess the environmental footprint of copper mining. J. Clean. Prod. 40, 118–128. https://doi.org/10.1016/j.jclepro.2012.09.027 ',\n",
       " 'Northey, S.A., Mudd, G.M., Saarivuori, E., Wessman-Jääskeläinen, H., Haque, N., 2016. Water footprinting and mining: Where are the limitations and opportunities? J. Clean. Prod. 135, 1098–1116. https://doi.org/10.1016/j.jclepro.2016.07.024 ',\n",
       " 'Onn, A.H., Woodley, A., 2014. A discourse analysis on how the sustainability agenda is defined within the mining industry. J. Clean. Prod. 84, 116–127. https://doi.org/10.1016/j.jclepro.2014.03.086 ',\n",
       " 'Parker, D.J., MNaughton, C.S., Sparks, G.A., 2016. Life Cycle Greenhouse Gas Emissions from Uranium Mining and Milling in Canada. Environ. Sci. Technol. 50, 9746–9753. https://doi.org/10.1021/acs.est.5b06072 ',\n",
       " 'Parvez Mahmud, M.A., Huda, N., Farjana, S.H., Asadnia, M., Lang, C., 2018. Recent Advances in Nanogenerator-Driven Self-Powered Implantable Biomedical Devices. Adv. Energy Mater. 8, 1–25. https://doi.org/10.1002/aenm.201701210 ',\n",
       " 'Pellegrino, C., Lodhia, S., 2012. Climate change accounting and the Australian mining industry: Exploring the links between corporate disclosure and the generation of legitimacy. J. Clean. Prod. 36, 68–82. https://doi.org/10.1016/j.jclepro.2012.02.022 ',\n",
       " 'Poinssot, C., Bourg, S., Ouvrier, N., Combernoux, N., Rostaing, C., Vargas-Gonzalez, M., Bruno, J., 2014. Assessment of the environmental footprint of nuclear energy systems. Comparison between closed and open fuel cycles. Energy 69, 199–211. https://doi.org/10.1016/j.energy.2014.02.069 ',\n",
       " 'Ranängen, H., Lindman, Å., 2017. A path towards sustainability for the Nordic mining industry. J. Clean. Prod. 151, 43–52. https://doi.org/10.1016/j.jclepro.2017.03.047 ',\n",
       " 'Raugei, M., Ulgiati, S., 2009. A novel approach to the problem of geographic allocation of environmental impact in Life Cycle Assessment and Material Flow Analysis. Ecol. Indic. 9, 1257–1264. https://doi.org/10.1016/j.ecolind.2009.04.001 ',\n",
       " 'Santero, N., Hendry, J., 2016. Harmonization of LCA methodologies for the metal and mining industry. Int. J. Life Cycle Assess. 21, 1543–1553. https://doi.org/10.1007/s11367-015-1022-4 ',\n",
       " 'Schneider, E., Carlsen, B., Tavrides, E., van der Hoeven, C., Phathanapirom, U., 2013. A top-down assessment of energy, water and land use in uranium mining, milling, and refining. Energy Econ. 40, 911–926. https://doi.org/10.1016/j.eneco.2013.08.006 ',\n",
       " 'Simons, A., Firth, S.K., 2011. Life-cycle assessment of a 100% solar fraction thermal supply to a European apartment building using water-based sensible heat storage. Energy Build. 43, 1231–1240. https://doi.org/10.1016/j.enbuild.2010.12.029 ',\n",
       " 'Sonter, L.J., Moran, C.J., Barrett, D.J., Soares-Filho, B.S., 2014. Processes of land use change in mining regions. J. Clean. Prod. 84, 494–501. https://doi.org/10.1016/j.jclepro.2014.03.084 ',\n",
       " 'Tost, M., Hitch, M., Chandurkar, V., Moser, P., Feiel, S., 2018. The state of environmental sustainability considerations in mining. J. Clean. Prod. 182, 969–977. https://doi.org/10.1016/j.jclepro.2018.02.051 ',\n",
       " 'Vintró, C., Sanmiquel, L., Freijo, M., 2014. Environmental sustainability in the mining sector: Evidence from Catalan companies. J. Clean. Prod. 84, 155–163. https://doi.org/10.1016/j.jclepro.2013.12.069 ',\n",
       " 'Warner, E.S., Heath, G.A., 2012. Life Cycle Greenhouse Gas Emissions of Nuclear Electricity Generation: Systematic Review and Harmonization. J. Ind. Ecol. 16, 73–92. https://doi.org/10.1111/j.1530-9290.2012.00472.x ',\n",
       " 'ASHRAE, Handbook of HVAC Applications, American Society of Heating, Refrigerating and Air Conditioning Engineers, Atlanta, GA, USA, 2003. ',\n",
       " 'Schmidt, M., Teichmann, T., “ Influence of the packing density of fine particles on ',\n",
       " 'structure, strength and durability of UHPC, “ International Symp. On Ultra High Performance Concrete, Kassel, Germany, September 13-15, 2004, pp. 313-323. ',\n",
       " 'Baby, F., Marchand, P., and Toutlemonde, F., “Shear Behavior of Ultrahigh Performance Fiber Reinforced Concrete Beams. II: Analysis and Design Provisions,” Journal of Structural Engineering, ASCE, 2014.140, pp. (04013112-1)-(04013112-11). ',\n",
       " 'ACI Committee 318, “ Building Code Requirements for Structural Concrete (ACI 318-14) and Commentary (ACI 318R-14),” American Concrete Institute, Farmington Hills, Mich., 2014. ',\n",
       " 'European Committee for Standardization, “Eurocode 2: Design of Concrete Structures- Part 1,” Brussels, December, 2004. ',\n",
       " 'Association Francaise du Genil Civil (AFGC),“ Ultra High-Performance Fiber Reinforced Concrete,” France, 2002. ',\n",
       " 'Association Francaise du Genil Civil (AFGC),“ Ultra High-Performance Fiber Reinforced Concrete,” France, 2013. ',\n",
       " 'Japan Society of Civil Engineers (JSCE), “ Recommendations for Design and Construction of Ultra High-Strength Fiber Reinforced Concrete Structures,” Concrete Engineering Series No. 82, March, 2008. ',\n",
       " \"ABAQUS: Abaqus analysis user's manual, Version 6.9, Dassault Systems Corp., Providence, RI, USA, 2009. \",\n",
       " 'Ali Sayigh. Renewable energy—the way forward. Appl Energy 1999;64(1):15-30. http://dx.doi.org/10.1016/s0306-2619(99)00117-8 ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oth_df.text.head(50).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
