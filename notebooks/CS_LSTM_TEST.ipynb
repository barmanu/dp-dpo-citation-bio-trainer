{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import git\n",
    "import mlflow.keras\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.preprocessing import *\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/joshib/cs_data/citation-bio-labelled-data-2020-06-08~15:13:57.766608/nlp/exps/output/2020-06-08~15:13:57.766608'\n",
    "#data_df = pd.read_csv(path+'/data-2020-06-08~15:14:01.185827.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with random embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 45s, sys: 3.4 s, total: 2min 49s\n",
      "Wall time: 3min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#train_data_path = '/home/joshib/cs_data/citation-bio-labelled-data-2020-06-09~06:30:11.273245/nlp/exps/output/2020-06-09~06:30:11.273245'\n",
    "#train_data_path = '/nlp/cs_data/2020-06-09~06:30:11.273245'\n",
    "train_data_path = '/nlp/cs_data_full/cs_data_train/'\n",
    "test_data_path  = '/nlp/cs_data_full/cs_data_test/'\n",
    "\n",
    "pattern = \"*#*#\"\n",
    "sentences = []\n",
    "sent_tags = []\n",
    "len_arr = []\n",
    "for fpath in os.listdir(train_data_path):\n",
    "    if fpath not in ['data-gen-config.json', 'data_generation_stats.csv'] and \".csv\" in fpath:\n",
    "        fpath = os.path.join(train_data_path, fpath)\n",
    "        df = pd.read_csv(fpath, index_col=0)\n",
    "        df.fillna(\"\\n\", axis=1, inplace=True)\n",
    "        len_arr.append(df.shape[0])\n",
    "        sentences.append(pattern.join(df.x))\n",
    "        sent_tags.append(pattern.join(df.y))\n",
    "        \n",
    "sentences_test = []\n",
    "sent_tags_test = []\n",
    "for fpath in os.listdir(test_data_path):\n",
    "    if fpath not in ['data-gen-config.json', 'data_generation_stats.csv'] and \".csv\" in fpath:\n",
    "        fpath = os.path.join(test_data_path, fpath)\n",
    "        df = pd.read_csv(fpath, index_col=0)\n",
    "        df.fillna(\"\\n\", axis=1, inplace=True)\n",
    "        len_arr.append(df.shape[0])\n",
    "        sentences_test.append(pattern.join(df.x))\n",
    "        sent_tags_test.append(pattern.join(df.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 26\n",
      "Max: 7272\n",
      "Mean: 1602\n",
      "Median: 1543\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(\"Min: %d\"%numpy.min(len_arr))\n",
    "print(\"Max: %d\"%numpy.max(len_arr))\n",
    "print(\"Mean: %d\"%numpy.mean(len_arr))\n",
    "print(\"Median: %d\"%numpy.median(len_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 11s, sys: 5.4 s, total: 3min 16s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Train\n",
    "maxlen = max(len_arr)\n",
    "X = [[w for w in s.split(pattern)] for s in sentences]\n",
    "y = [[p for p in t.split(pattern)] for t in sent_tags]\n",
    "\n",
    "new_X = []\n",
    "new_y = []\n",
    "for ind in range(len(X)):\n",
    "    new_seq = []\n",
    "    new_tag = []\n",
    "    for i in range(maxlen):\n",
    "        try:\n",
    "            new_seq.append(X[ind][i])\n",
    "            new_tag.append(y[ind][i])\n",
    "        except:\n",
    "            new_seq.append(\"PADword\")\n",
    "            new_tag.append(\"I-CIT\")\n",
    "    new_X.append(new_seq)\n",
    "    new_y.append(new_tag)\n",
    "\n",
    "### Test\n",
    "X_test = [[w for w in s.split(pattern)] for s in sentences_test]\n",
    "y_test = [[p for p in t.split(pattern)] for t in sent_tags_test]\n",
    "\n",
    "new_X_test = []\n",
    "new_y_test = []\n",
    "for ind in range(len(X_test)):\n",
    "    new_seq = []\n",
    "    new_tag = []\n",
    "    for i in range(maxlen):\n",
    "        try:\n",
    "            new_seq.append(X_test[ind][i])\n",
    "            new_tag.append(y_test[ind][i])\n",
    "        except:\n",
    "            new_seq.append(\"PADword\")\n",
    "            new_tag.append(\"I-CIT\")\n",
    "    new_X_test.append(new_seq)\n",
    "    new_y_test.append(new_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 100000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "#maxlen = 1000 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.4 s, sys: 1.45 s, total: 44.8 s\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tags = df.y.unique()\n",
    "tags2index = {'B-CIT': 1, 'I-CIT': 0}\n",
    "new_y_enc = np.array([[tags2index[w] for w in s] for s in new_y])\n",
    "new_y_test_enc = np.array([[tags2index[w] for w in s] for s in new_y_test])\n",
    "#y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tags2index[\"I-CIT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_orig, X_valid_orig, y_train, y_valid = train_test_split(new_X, new_y_enc, test_size=0.1, random_state=42)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=1)\n",
    "tokenizer.fit_on_texts(list(X_train_orig))\n",
    "X_train  = np.array(tokenizer.texts_to_sequences(X_train_orig))\n",
    "X_valid  = np.array(tokenizer.texts_to_sequences(X_valid_orig))\n",
    "X_test   = np.array(tokenizer.texts_to_sequences(new_X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28011, 7272)\n",
      "(3113, 7272)\n",
      "(7778, 7272)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with pre-trained fast text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min, sys: 4.27 s, total: 3min 4s\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_model = FastText.load_fasttext_format('/nlp/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n",
      "number of null word embeddings: 10209\n",
      "CPU times: user 7.91 s, sys: 236 ms, total: 8.14 s\n",
      "Wall time: 8.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_embedding_matrix(embeddings, nb_words, word_index, embed_dim):  # load embeddings\n",
    "    embeddings_index = {}\n",
    "    for word in embeddings.wv.vocab:\n",
    "        embeddings_index[word] = embeddings[word]\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    words_not_found = []\n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = load_embedding_matrix(wiki_model, max_features, tokenizer.word_index, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('embedding_matrix.npy', embedding_matrix)\n",
    "#embedding_matrix = np.load('./embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py:820: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if (isinstance(inputs, collections.Sequence)\n",
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:523: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  tensor_proto.tensor_content = nparray.tostring()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 7272, 300)         30000000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 7272, 50)          70200     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 7272, 1)           51        \n",
      "=================================================================\n",
      "Total params: 30,070,251\n",
      "Trainable params: 30,070,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 1.49 s, sys: 947 ms, total: 2.44 s\n",
      "Wall time: 5.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 300, weights=[embedding_matrix], trainable=True, input_length= maxlen))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1, activation=\"sigmoid\")))\n",
    "# model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(\n",
    "    optimizer=Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0,\n",
    "        epsilon=1e-05,\n",
    "        amsgrad=False,\n",
    "    ),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulate_ser_jer(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param keep_tag:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_true, y_pred=y_pred).ravel()\n",
    "    ser = 1.0\n",
    "    if (tp + fp) > 0.0:\n",
    "        ser = fp / float(tp + fp)\n",
    "    jer = 1.0\n",
    "    if (tp + fn) > 0.0:\n",
    "        jer = fn / float(tp + fn)\n",
    "    return ser, jer\n",
    "\n",
    "class Metrics(callbacks.Callback):\n",
    "    def __init__(self, train_data, validation_data):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_ser = []\n",
    "        self.val_jer = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        vx = self.validation_data[0]\n",
    "        vy = self.validation_data[1]\n",
    "        vx = np.array(vx)\n",
    "        pred_y = None\n",
    "        pred_y = self.model.predict(vx)\n",
    "        py = np.argmax(pred_y, axis=-1)\n",
    "        #vy = np.argmax(vy, axis=-1)\n",
    "        vy = np.array(vy).flatten()\n",
    "        py = np.array(py).flatten()\n",
    "        ser, jer = calulate_ser_jer(vy, py)\n",
    "        print(ser, jer)\n",
    "        self.val_ser.append(ser)\n",
    "        self.val_jer.append(jer)\n",
    "        logs[\"val_ser\"] = ser\n",
    "        logs[\"val_jer\"] = jer\n",
    "        print(f\"— val_ser: {ser} — val_jer: {jer}\")\n",
    "\n",
    "#         gold_x, gold_y, x = df_to_input()\n",
    "#         pred_y = self.model.predict(x)\n",
    "#         py = np.argmax(pred_y, axis=-1)[0]\n",
    "#         gy = np.argmax(gold_y, axis=-1)\n",
    "#         data = []\n",
    "#         for i in range(len(gold_x)):\n",
    "#             data.append({\"x\": gold_x[i], \"gold\": gy[i], \"pred\": py[i]})\n",
    "#         d = pd.DataFrame(data)\n",
    "#         d.to_csv(\"./\" + str(epoch) + \".predictions.on.sample.csv\")\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics([X_train, y_train], [X_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:718: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  if not isinstance(wrapped_dict, collections.Mapping):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438/438 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.99611.0 1.0\n",
      "— val_ser: 1.0 — val_jer: 1.0\n",
      "438/438 [==============================] - 380s 867ms/step - loss: 0.0210 - accuracy: 0.9961 - val_loss: 4.4917e-04 - val_accuracy: 0.9999 - val_ser: 1.0000 - val_jer: 1.0000\n",
      "Epoch 2/200\n",
      "438/438 [==============================] - ETA: 0s - loss: 1.6263e-04 - accuracy: 1.00001.0 1.0\n",
      "— val_ser: 1.0 — val_jer: 1.0\n",
      "438/438 [==============================] - 378s 864ms/step - loss: 1.6263e-04 - accuracy: 1.0000 - val_loss: 8.1088e-05 - val_accuracy: 1.0000 - val_ser: 1.0000 - val_jer: 1.0000\n",
      "Epoch 3/200\n",
      "142/438 [========>.....................] - ETA: 3:39 - loss: 6.8303e-05 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "early_stop= EarlyStopping(monitor='val_loss',patience=3,verbose=0,mode='min',restore_best_weights=False, min_delta=0.0001)\n",
    "history = model.fit(X_train, y_train, verbose=1, epochs=200, batch_size= 64, validation_data=(X_valid, y_valid), callbacks=[early_stop,metrics])\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "axes[0].plot(history.history['accuracy'])\n",
    "axes[0].plot(history.history['val_accuracy'])\n",
    "axes[0].title.set_text('model accuracy')\n",
    "axes[0].set_ylabel('accuracy')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "axes[1].plot(history.history['loss'])\n",
    "axes[1].plot(history.history['val_loss'])\n",
    "axes[1].title.set_text('model loss')\n",
    "axes[1].set_ylabel('loss')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].legend(['train', 'val'], loc='upper left')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_probs = model.predict(X_valid)\n",
    "valid_probs = valid_probs.reshape(valid_probs.shape[0], valid_probs.shape[1])\n",
    "valid_preds = np.where(valid_probs > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_jer = [calulate_ser_jer(i,j) for i,j in zip(y_valid, valid_preds)]\n",
    "accuracy = [accuracy_score(i,j) for i,j in zip(y_valid, valid_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SER: 0.000152\n",
      "Mean JER: 0.000096\n",
      "Mean Accuracy: 0.999998\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean SER: %0.6f\"%np.mean([i[0] for i in ser_jer]))\n",
    "print(\"Mean JER: %0.6f\"%np.mean([i[1] for i in ser_jer]))\n",
    "print(\"Mean Accuracy: %0.6f\"%np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 False positives: [1]\n",
      "['K.W.' 'Roth,' 'D.']\n",
      "247 False positives: [1]\n",
      "['Block' 'P' '(2007)']\n",
      "297 False negatives: [0]\n",
      "['USP' 'Technologies,']\n",
      "308 False positives: [664]\n",
      "['\\n' 'Conaire,' 'M.,']\n",
      "308 False negatives: [663]\n",
      "['\\n' '\\n' 'Conaire,']\n",
      "311 False positives: [396 401]\n",
      "['\\n' '(2017).' '\\n']\n",
      "['\\n' '(2010)' 'V21(7):1644–1646.']\n",
      "311 False negatives: [395 400]\n",
      "['\\n' '\\n' '(2017).']\n",
      "['\\n' '\\n' '(2010)']\n",
      "341 False positives: [71]\n",
      "['\\n' 'Conaire,' 'M.,']\n",
      "341 False negatives: [70]\n",
      "['\\n' '\\n' 'Conaire,']\n",
      "521 False positives: [2]\n",
      "['\\n' 'c' 'M,']\n",
      "676 False positives: [555]\n",
      "['\\n' 'Li' 'source']\n",
      "942 False positives: [236 241]\n",
      "['\\n' '(2017).' '\\n']\n",
      "['\\n' '(2010)' 'V21(7):1644–1646.']\n",
      "942 False negatives: [235 240]\n",
      "['\\n' '\\n' '(2017).']\n",
      "['\\n' '\\n' '(2010)']\n",
      "1130 False positives: [1]\n",
      "['Author' 'X' '(2008).']\n",
      "1138 False positives: [587]\n",
      "['\\n' 'L,' 'YS']\n",
      "1138 False negatives: [586]\n",
      "['\\n' '\\n' 'L,']\n",
      "1310 False positives: [868]\n",
      "['\\n' 'K' 'and']\n",
      "1504 False positives: [578]\n",
      "['\\n' 'L,' 'YS']\n",
      "1504 False negatives: [577]\n",
      "['\\n' '\\n' 'L,']\n",
      "1643 False negatives: [0]\n",
      "['Energy' '&']\n",
      "1809 False negatives: [0]\n",
      "['Fang,' 'C.L.,']\n",
      "2415 False positives: [2203]\n",
      "['\\n' '[' 'accessed']\n",
      "2536 False positives: [2]\n",
      "['\\n' 'C.' 'J.']\n",
      "2575 False positives: [772]\n",
      "['\\n' 'Edn.:' 'The']\n",
      "2652 False positives: [156]\n",
      "['\\n' '8.' '\\n']\n",
      "2755 False positives: [1]\n",
      "['Brasil.' 'LEI' 'No']\n",
      "2787 False negatives: [0]\n",
      "['Energy' 'storage-revolution']\n",
      "2898 False positives: [706]\n",
      "['\\n' 'Li' 'source']\n",
      "2912 False negatives: [0]\n",
      "['Fang,' 'T.,']\n",
      "2985 False negatives: [0]\n",
      "['S.,' 'Von']\n",
      "2998 False negatives: [0]\n",
      "['Health' 'Information']\n",
      "3053 False positives: [1]\n",
      "['X.B.' 'Liu' 'and']\n"
     ]
    }
   ],
   "source": [
    "count_valid = 0\n",
    "for ind in range(len(valid_preds)):\n",
    "    pred = valid_preds[ind]\n",
    "    true = y_valid[ind]\n",
    "    if (true == pred).all():\n",
    "        pass\n",
    "    else:\n",
    "        count_valid += 1\n",
    "        fp_ind = np.where((pred == 1) & (true == 0))[0]\n",
    "        fn_ind = np.where((pred == 0) & (true == 1))[0]\n",
    "        if len(fp_ind) > 0:\n",
    "            #print(fp_ind)\n",
    "            print(ind, 'False positives:', fp_ind)\n",
    "            for x in fp_ind:\n",
    "                #print(x)\n",
    "                print(np.array(X_valid_orig[ind])[max(0, x-1):x+2])\n",
    "        if len(fn_ind) > 0:\n",
    "            print(ind, 'False negatives:', fn_ind)\n",
    "            for x in fn_ind:\n",
    "                print(np.array(X_valid_orig[ind])[max(0, x-1):x+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind in range(5):\n",
    "#     print(ind, np.array(X_valid_orig)[ind][np.where(y_valid[ind] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 mistakes out of total 3113 citations\n"
     ]
    }
   ],
   "source": [
    "print(\"%d mistakes out of total %d citations\"%(count_valid, len(valid_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict(X_test)\n",
    "test_probs = test_probs.reshape(test_probs.shape[0], test_probs.shape[1])\n",
    "test_preds = np.where(test_probs > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_jer = [calulate_ser_jer(i,j) for i,j in zip(new_y_test_enc, test_preds)]\n",
    "accuracy = [accuracy_score(i,j) for i,j in zip(new_y_test_enc, test_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SER: 0.000102\n",
      "Mean JER: 0.000076\n",
      "Mean Accuracy: 0.999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean SER: %0.6f\"%np.mean([i[0] for i in ser_jer]))\n",
    "print(\"Mean JER: %0.6f\"%np.mean([i[1] for i in ser_jer]))\n",
    "print(\"Mean Accuracy: %0.6f\"%np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 False positives:\n",
      "['\\n' 'Connor,' 'P.,']\n",
      "117 False negatives: [0]\n",
      "['Health' 'MDo.']\n",
      "305 False positives:\n",
      "['\\n' 'G.V.' 'Samsonov,']\n",
      "305 False negatives: [1240]\n",
      "['\\n' '\\n' 'G.V.']\n",
      "372 False negatives: [0]\n",
      "['Luyckx,' 'K.,']\n",
      "515 False negatives: [0]\n",
      "['Kwast,' 'van']\n",
      "592 False positives:\n",
      "['\\n' 'Jr' 'ER,']\n",
      "774 False negatives: [0]\n",
      "['Energy' 'efficient']\n",
      "804 False positives:\n",
      "['\\n' '62' '(2010),']\n",
      "807 False positives:\n",
      "['\\n' '[' 'Online']\n",
      "844 False negatives: [0]\n",
      "['Araki,' 'M.,']\n",
      "976 False negatives: [0]\n",
      "['Energy' 'Research']\n",
      "990 False negatives: [0]\n",
      "['Health' 'MDo.']\n",
      "1353 False positives:\n",
      "['\\n' 'Nanofiltration—An' 'Overview']\n",
      "1353 False negatives: [1174]\n",
      "['\\n' '\\n' 'Nanofiltration—An']\n",
      "1368 False negatives: [655]\n",
      "['\\n' '[22]' 'A.']\n",
      "1932 False positives:\n",
      "['\\n' 'für' 'Statistik,']\n",
      "1939 False negatives: [0]\n",
      "['K.E.' 'Kaloush,']\n",
      "2085 False positives:\n",
      "['\\n' '11' 'June']\n",
      "2203 False negatives: [0]\n",
      "['.' '\\n']\n",
      "2253 False positives:\n",
      "['\\n' 'Yin,' 'X.G.']\n",
      "2367 False negatives: [0]\n",
      "['in' 'regulating']\n",
      "2376 False negatives: [0]\n",
      "['.' '\\n']\n",
      "3043 False positives:\n",
      "['\\n' 'are' 'obtained']\n",
      "3187 False negatives: [0]\n",
      "['NIST,' 'National']\n",
      "3248 False positives:\n",
      "['\\n' 'G:' 'Evaluation']\n",
      "3248 False negatives: [429]\n",
      "['\\n' '\\n' 'G:']\n",
      "3267 False negatives: [0]\n",
      "['SL' 'Ishman,']\n",
      "3411 False positives:\n",
      "['durability.' 'KL' 'Scrivener,']\n",
      "3466 False positives:\n",
      "['Hu,' 'Q.' 'et']\n",
      "3631 False positives:\n",
      "['\\n' '(2017).' '\\n']\n",
      "['\\n' '(2010)' 'V21(7):1644–1646.']\n",
      "3631 False negatives: [704 710]\n",
      "['\\n' '\\n' '(2017).']\n",
      "['\\n' '\\n' '(2010)']\n",
      "3759 False positives:\n",
      "['\\n' 'L,' 'YS']\n",
      "3759 False negatives: [648]\n",
      "['\\n' '\\n' 'L,']\n",
      "3790 False positives:\n",
      "['\\n' 'C.' 'B.,']\n",
      "3893 False positives:\n",
      "['\\n' 'Edn.:' 'The']\n",
      "4119 False negatives: [0]\n",
      "['SL' 'Ishman,']\n",
      "4152 False positives:\n",
      "['Auerbach,' 'Carl' 'F,']\n",
      "4201 False positives:\n",
      "['\\n' 'L,' 'YS']\n",
      "4201 False negatives: [1350]\n",
      "['\\n' '\\n' 'L,']\n",
      "4252 False positives:\n",
      "['IS:' '2386' '(Part']\n",
      "4366 False positives:\n",
      "['\\n' '6.' '\\n']\n",
      "4437 False positives:\n",
      "['\\n' 'are' 'obtained']\n",
      "4692 False positives:\n",
      "['\\n' 'K.' '(2007).']\n",
      "4738 False positives:\n",
      "['M.-S.' 'Balogun,' 'Y.']\n",
      "4865 False positives:\n",
      "['\\n' '(' 'In']\n",
      "4916 False positives:\n",
      "['\\n' 'Photoproduction.' 'Technical']\n",
      "5004 False positives:\n",
      "['Xu,' 'X' 'w;']\n",
      "5027 False positives:\n",
      "['\\n' 'G.V.' 'Samsonov,']\n",
      "5027 False negatives: [240]\n",
      "['\\n' '\\n' 'G.V.']\n",
      "5539 False positives:\n",
      "['\\n' 'Edn.:' 'The']\n",
      "5897 False negatives: [0]\n",
      "['A.,' 'F.']\n",
      "5986 False negatives: [0]\n",
      "['“Wind' 'Power']\n",
      "6135 False positives:\n",
      "['\\n' 'J.' 'M.,']\n",
      "6563 False negatives: [0]\n",
      "['T.,' 'Umahara,']\n",
      "6950 False negatives: [0]\n",
      "['N.,' 'Hodnik,']\n",
      "7065 False negatives: [0]\n",
      "['J.,' 'Nauman;']\n",
      "7104 False positives:\n",
      "['\\n' 'J.L.R.,' 'Jenkinson,']\n",
      "7162 False positives:\n",
      "['\\n' '“Adhésifs-Mouillabilité-Détermination' 'par']\n",
      "7217 False positives:\n",
      "['U.H.' 'Weidle,' 'D.']\n",
      "7332 False positives:\n",
      "['\\n' 'Edn.:' 'The']\n",
      "7344 False positives:\n",
      "['analysis:' 'A' 'regression-based']\n",
      "7454 False positives:\n",
      "['\\n' 'B.' '\\n']\n",
      "7558 False negatives: [0]\n",
      "['Energy' 'Institute.']\n",
      "7766 False positives:\n",
      "['\\n' '[' 'WWW']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for ind in range(len(test_preds)):\n",
    "    pred = test_preds[ind]\n",
    "    true = new_y_test_enc[ind]\n",
    "    if (true == pred).all():\n",
    "        pass\n",
    "    else:\n",
    "        count += 1\n",
    "        fp_ind = np.where((pred == 1) & (true == 0))[0]\n",
    "        fn_ind = np.where((pred == 0) & (true == 1))[0]\n",
    "        if len(fp_ind) > 0:\n",
    "            #print(fp_ind)\n",
    "            print(ind, 'False positives:')\n",
    "            for x in fp_ind:\n",
    "                #print(x)\n",
    "                print(np.array(new_X_test[ind])[max(0, x-1):x+2])\n",
    "        if len(fn_ind) > 0:\n",
    "            print(ind, 'False negatives:', fn_ind)\n",
    "            for x in fn_ind:\n",
    "                print(np.array(new_X_test[ind])[max(0, x-1):x+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ind in range(5):\n",
    "#     print(ind, np.array(new_X_test)[ind][np.where(new_y_test_enc[ind] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 mistakes out of total 7778 citations\n"
     ]
    }
   ],
   "source": [
    "print(\"%d mistakes out of total %d citations\"%(count, len(test_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras with random embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(max_features, embed_size, input_length=maxlen))\n",
    "# model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
    "# #model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "# model.add(TimeDistributed(Dense(1, activation=\"sigmoid\")))\n",
    "# # # model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(\n",
    "#         learning_rate=0.005,\n",
    "#         beta_1=0.0,\n",
    "#         beta_2=0.0,\n",
    "#         epsilon=1e-05,\n",
    "#         amsgrad=False,\n",
    "#     ),\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, verbose=1, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras.models import *\n",
    "# import tensorflow as tf\n",
    "\n",
    "# X = np.random.randint(1000, size=(10000, 80))\n",
    "# y = np.random.choice([0,1], size=10000)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(1000, 50, input_length = 80))\n",
    "# model.add(LSTM(10))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# print ('Compiling...')\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='sgd',\n",
    "#               metrics=['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_model['[']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x = new_X_test[0:5]\n",
    "temp_y = new_y_test_enc[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x_new = []\n",
    "for sent in temp_x:\n",
    "    sent_new = []\n",
    "    for word in sent:\n",
    "        if word == '\\n':\n",
    "            random.shuffle(stops)\n",
    "            sent_new.append(stops[0])\n",
    "        else:\n",
    "            sent_new.append(word)\n",
    "    temp_x_new.append(sent_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_temp = np.array(tokenizer.texts_to_sequences(temp_x_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_probs = model.predict(X_test_temp)\n",
    "temp_probs = temp_probs.reshape(temp_probs.shape[0], temp_probs.shape[1])\n",
    "temp_preds = np.where(temp_probs > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_jer = [calulate_ser_jer(i,j) for i,j in zip(temp_y, temp_preds)]\n",
    "accuracy = [accuracy_score(i,j) for i,j in zip(temp_y, temp_preds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean SER: 0.000000\n",
      "Mean JER: 0.983388\n",
      "Mean Accuracy: 0.991474\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean SER: %0.6f\"%np.mean([i[0] for i in ser_jer]))\n",
    "print(\"Mean JER: %0.6f\"%np.mean([i[1] for i in ser_jer]))\n",
    "print(\"Mean Accuracy: %0.6f\"%np.mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 47 0 46\n",
      "1 1 65 0 64\n",
      "2 1 79 0 78\n",
      "3 1 75 0 74\n",
      "4 1 49 0 48\n"
     ]
    }
   ],
   "source": [
    "for ind in range(5):\n",
    "    pred = temp_preds[ind]\n",
    "    true = temp_y[ind]\n",
    "    print(ind, len(np.where(pred == 1)[0]), len(np.where(true == 1)[0]), len(np.where((pred == 1) & (true == 0))[0]), len(np.where((pred == 0) & (true == 1))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(temp_preds[0] ==1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_x_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,   47,   69,   88,  119,  134,  154,  191,  220,  270,  290,\n",
       "         313,  343,  366,  391,  448,  485,  529,  569,  588,  621,  650,\n",
       "         684,  717,  739,  767,  801,  832,  857,  883,  906,  937,  968,\n",
       "        1004, 1038, 1069, 1116, 1155, 1186, 1237, 1275, 1320, 1348, 1388,\n",
       "        1424, 1467, 1495]),)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(temp_y[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]),)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(temp_preds[0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = temp_preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(bla == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testy\n"
     ]
    }
   ],
   "source": [
    "print('testy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
