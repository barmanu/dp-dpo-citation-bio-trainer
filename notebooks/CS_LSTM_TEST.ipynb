{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/joshib/.cache/pypoetry/virtualenvs/dp-dpo-citation-bio-trainer-FaGiNfZ9-py3.7/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import git\n",
    "import mlflow.keras\n",
    "import os, sys\n",
    "import numpy as np\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.preprocessing import *\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import *\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "sys.path.append('../citation_bio_trainer')\n",
    "#from feature.SpacyFeaturizer import get_spacy_feats_from_text\n",
    "from feature.Featurizer import Featurizer\n",
    "from util.Utils import calulate_ser_jer, load_from_folder, pad_sequences, load_embedding_matrix, evaluate, log_mlflow_results\n",
    "import warnings\n",
    "from model.FTLSTM import Metrics, calulate_ser_jer\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_value= 0\n",
    "\n",
    "# import os\n",
    "# os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "# import random\n",
    "# random.seed(seed_value)\n",
    "\n",
    "# # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "# import numpy as np\n",
    "# np.random.seed(seed_value)\n",
    "\n",
    "# # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "# import tensorflow as tf\n",
    "# tf.compat.v1.set_random_seed(seed_value)\n",
    "\n",
    "# # 5. Configure a new global `tensorflow` session\n",
    "# from tensorflow.compat.v1.keras import backend as K\n",
    "# session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "# sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/joshib/cs_data/citation-bio-labelled-data-2020-06-08~15:13:57.766608/nlp/exps/output/2020-06-08~15:13:57.766608'\n",
    "#data_df = pd.read_csv(path+'/data-2020-06-08~15:14:01.185827.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with random embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 29s, sys: 878 ms, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_path = '/nlp/cs_mixed_20k/cs_mixed_20k_train/'\n",
    "test_data_path  = '/nlp/cs_mixed_20k/cs_mixed_20k_test/'\n",
    "eval_without_intra_newline_path  = '/nlp/eval_without_intra_newline/'\n",
    "eval_with_intra_newline_path     = '/nlp/eval_with_intra_newline/'\n",
    "\n",
    "sentences_train, sent_tags_train = load_from_folder(train_data_path)\n",
    "sentences_test, sent_tags_test = load_from_folder(test_data_path)\n",
    "sentences_eval1, sent_tags_eval1 = load_from_folder(eval_without_intra_newline_path)\n",
    "#sentences_eval2, sent_tags_eval2 = load_from_folder(eval_with_intra_newline_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(sentences_train, sent_tags_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_config = {'max_vocab':100000,\n",
    "               'lstm_feats':True, \n",
    "               'spacy_feats':True, \n",
    "               'google_feats': False, \n",
    "               'parscit_feats': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train ...\n",
      "total partitions = 15\n",
      "total partitions = 15\n",
      "loading valid ...\n",
      "total partitions = 1\n",
      "loading test ...\n",
      "total partitions = 1\n",
      "loading evals ...\n",
      "total partitions = 1\n",
      "CPU times: user 13min 15s, sys: 28.8 s, total: 13min 43s\n",
      "Wall time: 21min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "featurizer = Featurizer(feat_config)\n",
    "print(\"loading train ...\")\n",
    "train_dict, tokenizer, maxlen = featurizer.fit_transform(xtrain, ytrain)\n",
    "print(\"loading valid ...\")\n",
    "valid_dict = featurizer.transform(xvalid, yvalid)\n",
    "print(\"loading test ...\")\n",
    "test_dict  = featurizer.transform(sentences_test, sent_tags_test)\n",
    "print(\"loading evals ...\")\n",
    "eval_dict1  = featurizer.transform(sentences_eval1, sent_tags_eval1)\n",
    "#eval_dict2  = featurizer.transform(sentences_eval2, sent_tags_eval2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural parscit features (temporary fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from feature.ParsCitLSTM import ParsCitLSTM\n",
    "# c = {\n",
    "#         \"model_file\": \"/nlp/parscit/parscit-29-latest.h5\",\n",
    "#         \"label_dict_file\": \"/nlp/parscit/labels.json\",\n",
    "#         \"tfhub_model_dir\": \"/nlp/parscit/resource/\"}\n",
    "# model = ParsCitLSTM(model_config=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_df = pd.DataFrame([])\n",
    "# valid_df = pd.DataFrame([])\n",
    "# test_df = pd.DataFrame([])\n",
    "# eval1_df = pd.DataFrame([])\n",
    "# eval2_df = pd.DataFrame([])\n",
    "\n",
    "# train_df['text'] = np.array(xtrain, dtype='object')\n",
    "# valid_df['text'] = np.array(xvalid, dtype='object')\n",
    "# test_df['text'] = np.array(sentences_test, dtype='object')\n",
    "# eval1_df['text'] = np.array(sentences_eval1, dtype='object')\n",
    "# eval2_df['text'] = np.array(sentences_eval2, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 598 ms, sys: 865 ms, total: 1.46 s\n",
      "Wall time: 1.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_parscit = model.get_parscit_blocks(train_df)\n",
    "# valid_parscit = model.get_parscit_blocks(valid_df)\n",
    "# test_parscit  = model.get_parscit_blocks(test_df)\n",
    "# eval1_parscit = model.get_parscit_blocks(eval1_df)\n",
    "# eval2_parscit = model.get_parscit_blocks(eval2_df)\n",
    "train_parscit = pd.read_pickle('/nlp/temp/train_parscit.pickle')\n",
    "valid_parscit = pd.read_pickle('/nlp/temp/valid_parscit.pickle')\n",
    "test_parscit = pd.read_pickle('/nlp/temp/test_parscit.pickle')\n",
    "eval1_parscit = pd.read_pickle('/nlp/temp/eval1_parscit.pickle')\n",
    "#eval2_parscit = pd.read_pickle('/nlp/temp/eval2_parscit.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dummy_feats(df, maxlen):\n",
    "    parscit = df.copy()\n",
    "    parscit_feats = list(parscit['parscit_feat']) \n",
    "    parscit_padded = []\n",
    "    for ind in range(len(parscit_feats)):\n",
    "        parscit_mask = np.zeros((maxlen, 14), dtype='int8')\n",
    "        if len(parscit_feats[ind]) <= maxlen:\n",
    "            parscit_mask[0:len(parscit_feats[ind]), :] = parscit_feats[ind][:]\n",
    "        else:\n",
    "            parscit_mask[:] = parscit_feats[ind][0:maxlen,:]\n",
    "        parscit_padded.append(parscit_mask)\n",
    "    parscit_arr = np.array([i.tolist() for i in parscit_padded])\n",
    "    return parscit_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 5s, sys: 9 s, total: 4min 14s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_parscit_arr = pad_dummy_feats(train_parscit, maxlen)\n",
    "valid_parscit_arr = pad_dummy_feats(valid_parscit, maxlen)\n",
    "test_parscit_arr  = pad_dummy_feats(test_parscit, maxlen)\n",
    "eval1_parscit_arr = pad_dummy_feats(eval1_parscit, maxlen)\n",
    "#eval2_parscit_arr = pad_dummy_feats(eval2_parscit, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras with pre-trained fast text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'bi-lstm':True,\n",
    "                'lstm': {'use':True, 'num': 1, 'units':50},\n",
    "                'dense':{'use':False, 'num': 1, 'units':50},\n",
    "                'trainable': True,\n",
    "                'optimizer': Adam(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.0,\n",
    "        beta_2=0.0,\n",
    "        epsilon=1e-05,\n",
    "        amsgrad=False,\n",
    "    ),\n",
    "                'output_activation' : 'sigmoid', \n",
    "                'batch_size': 16,\n",
    "                'aux_feats': {'use':True, 'dim':26}\n",
    "                \n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 20s, sys: 5.32 s, total: 2min 25s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wiki_model = FastText.load_fasttext_format('/nlp/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000000 word vectors.\n",
      "number of null word embeddings: 9769\n",
      "CPU times: user 7.89 s, sys: 304 ms, total: 8.19 s\n",
      "Wall time: 8.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix = load_embedding_matrix(wiki_model, feat_config['max_vocab'], tokenizer.word_index, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time ### old code reported in mlflow\n",
    "# input = Input(shape=(maxlen,))\n",
    "# embed = Embedding(input_dim=feat_config['max_vocab'], input_length=maxlen, output_dim=300, weights=[embedding_matrix], trainable=model_config['trainable'])(input)\n",
    "# lstm = LSTM(model_config['lstm_units'], return_sequences=True)(embed)\n",
    "# aux_feats = Input(shape=(maxlen,26))\n",
    "# conc = Concatenate([lstm, aux_feats])\n",
    "# td = TimeDistributed(Dense(1, activation=model_config['output_activation']))(lstm)\n",
    "# model = Model(inputs=[input, aux_feats], outputs=td)\n",
    "# model.compile(optimizer=model_config['optimizer'],\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concatenate_1/Identity:0' shape=(None, 3861, 76) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"multi-input-model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         [(None, 3861)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 3861, 300)    30000000    main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 3861, 50)     70200       embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary_input (InputLayer)    [(None, 3861, 26)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 3861, 76)     0           lstm_16[0][0]                    \n",
      "                                                                 auxiliary_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 3861, 512)    39424       concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 3861, 1)      513         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 30,110,137\n",
      "Trainable params: 30,110,137\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "CPU times: user 900 ms, sys: 250 ms, total: 1.15 s\n",
      "Wall time: 536 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inp = Input(shape=(maxlen,), name='main_input')\n",
    "embed = Embedding(input_dim=feat_config['max_vocab'], input_length=maxlen, output_dim=300, weights=[embedding_matrix], trainable=model_config['trainable'])(inp)\n",
    "lstm = LSTM(model_config['lstm']['units'], return_sequences=True)(embed)\n",
    "aux_feats = Input(shape=(maxlen,26), name='auxiliary_input')\n",
    "conc = Concatenate()([lstm, aux_feats])\n",
    "y =  Dense(512)(conc)\n",
    "out = TimeDistributed(Dense(1, activation=model_config['output_activation'], name='output'))(y)\n",
    "model = Model(inputs=[inp, aux_feats], outputs=out, name='multi-input-model')\n",
    "model.compile(optimizer=model_config['optimizer'],\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "def calulate_ser_jer(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param keep_tag:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_true, y_pred=y_pred).ravel()\n",
    "    ser = 1.0\n",
    "    if (tp + fp) > 0.0:\n",
    "        ser = fp / float(tp + fp)\n",
    "    jer = 1.0\n",
    "    if (tp + fn) > 0.0:\n",
    "        jer = fn / float(tp + fn)\n",
    "    return ser, jer\n",
    "\n",
    "class Metrics(callbacks.Callback):\n",
    "    def __init__(self, train_data, validation_data):\n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.train_data = train_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_ser = []\n",
    "        self.val_jer = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        vx = self.validation_data[0]\n",
    "        vy = self.validation_data[1]\n",
    "        vx = np.array(vx)\n",
    "        pred_y = None\n",
    "        pred_y = self.model.predict(vx)\n",
    "        py = np.argmax(pred_y, axis=-1)\n",
    "        #vy = np.argmax(vy, axis=-1)\n",
    "        vy = np.array(vy).flatten()\n",
    "        py = np.array(py).flatten()\n",
    "        ser, jer = calulate_ser_jer(vy, py)\n",
    "        print(ser, jer)\n",
    "        self.val_ser.append(ser)\n",
    "        self.val_jer.append(jer)\n",
    "        logs[\"val_ser\"] = ser\n",
    "        logs[\"val_jer\"] = jer\n",
    "        print(f\"— val_ser: {ser} — val_jer: {jer}\")\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = Metrics([train_dict['lstm_feats'], train_dict['labels']], [valid_dict['lstm_feats'], valid_dict['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aux = np.concatenate((train_dict['spacy_num_feats'], train_parscit_arr), axis=-1)\n",
    "valid_aux = np.concatenate((valid_dict['spacy_num_feats'], valid_parscit_arr), axis=-1)\n",
    "test_aux  = np.concatenate((test_dict['spacy_num_feats'], test_parscit_arr), axis=-1)\n",
    "eval1_aux = np.concatenate((eval_dict1['spacy_num_feats'], eval1_parscit_arr), axis=-1)\n",
    "#eval2_aux = np.concatenate((eval_dict2['spacy_num_feats'], eval2_parscit_arr), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "early_stop= EarlyStopping(monitor='val_loss',patience=3,verbose=0,mode='min',restore_best_weights=False, min_delta=0.0001)\n",
    "history = model.fit([train_dict['lstm_feats'], train_aux], train_dict['labels'], verbose=1, epochs=100, batch_size= model_config['batch_size'], \\\n",
    "                    validation_data=([valid_dict['lstm_feats'], valid_aux], valid_dict['labels']), callbacks=[early_stop], shuffle=False)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "axes[0].plot(history.history['accuracy'])\n",
    "axes[0].plot(history.history['val_accuracy'])\n",
    "axes[0].title.set_text('model accuracy')\n",
    "axes[0].set_ylabel('accuracy')\n",
    "axes[0].set_xlabel('epoch')\n",
    "axes[0].legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "axes[1].plot(history.history['loss'])\n",
    "axes[1].plot(history.history['val_loss'])\n",
    "axes[1].title.set_text('model loss')\n",
    "axes[1].set_ylabel('loss')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].legend(['train', 'val'], loc='upper left')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.4 s, sys: 1.58 s, total: 12 s\n",
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_probs = model.predict([valid_dict['lstm_feats'], valid_aux])\n",
    "valid_probs = valid_probs.reshape(valid_probs.shape[0], valid_probs.shape[1])\n",
    "valid_preds = np.where(valid_probs > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 64.7 ms, total: 12.8 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_valid = evaluate(valid_dict['labels'].tolist(), valid_preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1758,\n",
       " 'mean_ser': 0.002805715631041411,\n",
       " 'mean_jer': 0.004535388148081998,\n",
       " 'mean_acc': 0.9999575699234403,\n",
       " 'num_mistakes': 221}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_valid = 0\n",
    "for ind in range(len(valid_preds)):\n",
    "    pred = valid_preds[ind]\n",
    "    true = valid_dict['labels'][ind]\n",
    "    if (true == pred).all():\n",
    "        pass\n",
    "    else:\n",
    "        count_valid += 1\n",
    "        fp_ind = np.where((pred == 1) & (true == 0))[0]\n",
    "        fn_ind = np.where((pred == 0) & (true == 1))[0]\n",
    "        if len(fp_ind) > 0:\n",
    "            #print(fp_ind)\n",
    "            print(ind, 'False positives:', fp_ind)\n",
    "            for x in fp_ind:\n",
    "                #print(x)\n",
    "                print(np.array(xvalid[ind].split(\" \"))[max(0, x-3):x+4])\n",
    "        if len(fn_ind) > 0:\n",
    "            print(ind, 'False negatives:', fn_ind)\n",
    "            for x in fn_ind:\n",
    "                print(np.array(xvalid[ind].split(\" \"))[max(0, x-3):x+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 3.67 s, total: 20.3 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "keras_model = mlflow.keras.load_model(\"s3://caps-s3-mlflow/artifacts/4/e814b3fd2d854c1a83c5998cea627b59/artifacts/models\")\n",
    "test_probs = keras_model.predict([test_dict['lstm_feats'], test_aux])\n",
    "test_probs = test_probs.reshape(test_probs.shape[0], test_probs.shape[1])\n",
    "test_preds = np.where(test_probs > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 8.08 ms, total: 14 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_test = evaluate(test_dict['labels'].tolist(), test_preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1953,\n",
       " 'mean_ser': 0.003022608534831472,\n",
       " 'mean_jer': 0.0037607236813184367,\n",
       " 'mean_acc': 0.9999632651962401,\n",
       " 'num_mistakes': 238}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1953,\n",
       " 'mean_ser': 0.003022608534831472,\n",
       " 'mean_jer': 0.0037607236813184367,\n",
       " 'mean_acc': 0.9999632651962401,\n",
       " 'num_mistakes': 238}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'dataset':'cs_mixed_20k', 'data_split':'test'}\n",
    "opt = model_config['optimizer']\n",
    "model_config['optimizer'] = str(opt.get_config())\n",
    "log_mlflow_results(model, result_test, feat_config, model_config, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for ind in range(len(test_preds)):\n",
    "    pred = test_preds[ind]\n",
    "    true = new_y_test_enc[ind]\n",
    "    if (true == pred).all():\n",
    "        pass\n",
    "    else:\n",
    "        count += 1\n",
    "        fp_ind = np.where((pred == 1) & (true == 0))[0]\n",
    "        fn_ind = np.where((pred == 0) & (true == 1))[0]\n",
    "        if len(fp_ind) > 0:\n",
    "            #print(fp_ind)\n",
    "            print(ind, 'False positives:', fp_ind)\n",
    "            for x in fp_ind:\n",
    "                #print(x)\n",
    "                print(np.array(new_X_test[ind])[max(0, x-3):x+4])\n",
    "        if len(fn_ind) > 0:\n",
    "            print(ind, 'False negatives:', fn_ind)\n",
    "            for x in fn_ind:\n",
    "                print(np.array(new_X_test[ind])[max(0, x-3):x+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras_model = mlflow.keras.load_model(\"s3://caps-s3-mlflow/artifacts/4/e42c50d933ff49fb826dfe5874fa6d51/artifacts/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_probs1 = keras_model.predict([eval_dict1['lstm_feats'], eval1_aux])\n",
    "eval_probs1 = eval_probs1.reshape(eval_probs1.shape[0], eval_probs1.shape[1])\n",
    "eval_preds1 = np.where(eval_probs1 > 0.5, 1, 0)\n",
    "result_eval1 = evaluate(eval_dict1['labels'].tolist(), eval_preds1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 1328,\n",
       " 'mean_ser': 7.586536749363777e-05,\n",
       " 'mean_jer': 0.0033384519576073984,\n",
       " 'mean_acc': 0.9999648945432079,\n",
       " 'num_mistakes': 68}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_eval1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_probs2 = model.predict([eval_dict2['lstm_feats'], eval_dict2['spacy_num_feats']])\n",
    "eval_probs2 = eval_probs2.reshape(eval_probs2.shape[0], eval_probs2.shape[1])\n",
    "eval_preds2 = np.where(eval_probs2 > 0.5, 1, 0)\n",
    "result_eval2 = evaluate(eval_dict2['labels'].tolist(), eval_preds2.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_valid = 0\n",
    "for ind in range(len(eval1_aux)):\n",
    "    pred = eval_preds1[ind]\n",
    "    true = eval_dict1['labels'][ind]\n",
    "    if (true == pred).all():\n",
    "        pass\n",
    "    else:\n",
    "        count_valid += 1\n",
    "        fp_ind = np.where((pred == 1) & (true == 0))[0]\n",
    "        fn_ind = np.where((pred == 0) & (true == 1))[0]\n",
    "        if len(fp_ind) > 0:\n",
    "            #print(fp_ind)\n",
    "            print(ind, 'False positives:', fp_ind)\n",
    "            for x in fp_ind:\n",
    "                #print(x)\n",
    "                print(np.array(sentences_eval1[ind].split(\" \"))[max(0, x-3):x+4])\n",
    "        if len(fn_ind) > 0:\n",
    "            print(ind, 'False negatives:', fn_ind)\n",
    "            for x in fn_ind:\n",
    "                print(np.array(sentences_eval1[ind].split(\" \"))[max(0, x-3):x+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save eval results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3861"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(sentences_eval1[ind].split(\" \")), maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "eval_path_dict = {}\n",
    "for fpath in os.listdir(eval_without_intra_newline_path):\n",
    "    if fpath not in ['data-gen-config.json', 'data_generation_stats.csv'] and \".csv\" in fpath:\n",
    "        eval_path_dict[ind] = fpath\n",
    "        ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3861"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(len(sentences_eval1[75].split(\" \")), maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3861"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_preds1[75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_folder_path = '/nlp/eval_data_retok_predictions_f3a9cc61e30f460499b95a4c2b7957ae'\n",
    "if not os.path.exists(eval_folder_path):\n",
    "    os.makedirs(eval_folder_path)\n",
    "for ind in range(len(eval_preds1)):\n",
    "    #print(ind)\n",
    "    df = pd.DataFrame([], columns=['x', 'y'])\n",
    "    seq_len = min(len(sentences_eval1[ind].split(\" \")), maxlen)\n",
    "    df['x'] = sentences_eval1[ind].split(\" \")[0:seq_len]\n",
    "    df['y'] = eval_preds1[ind][0:seq_len]\n",
    "    df.to_csv(os.path.join(eval_folder_path, eval_path_dict[ind][0:-4] + '_pred.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/08/14 10:53:26 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under s3://caps-s3-mlflow/artifacts/4/f3a9cc61e30f460499b95a4c2b7957ae/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the  tracking store. If logging to a mlflow server via REST, consider  upgrading the server version to MLflow 1.7.0 or above.\n"
     ]
    }
   ],
   "source": [
    "tags = {'dataset':'cs_mixed_20k', 'data_split':'eval_with_intra_newline_path'}\n",
    "#opt = model_config['optimizer']\n",
    "#model_config['optimizer'] = str(opt.get_config())\n",
    "log_mlflow_results(keras_model, result_eval1, feat_config, model_config, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train[0][0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature.ParsCitLSTM import ParsCitLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/nlp/parscit/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([])\n",
    "df['text'] = np.array(sentences_train[0:100], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_out = model.dask_predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# import dask.dataframe as dd\n",
    "# parscit_df = model.get_parscit_dask(df, blocksize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_train[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x,y,z = model.predict(sentences_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in zip(x,y):\n",
    "#     print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(c['model_file'])\n",
    "# model._name='parscit_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = np.random.randint(10, size=(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([], columns=['num'])\n",
    "df['num'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "train_parscit = pd.read_pickle('/nlp/temp/train_parscit.pickle')\n",
    "valid_parscit = pd.read_pickle('/nlp/temp/valid_parscit.pickle')\n",
    "test_parscit = pd.read_pickle('/nlp/temp/test_parscit.pickle')\n",
    "eval1_parscit = pd.read_pickle('/nlp/temp/eval1_parscit.pickle')\n",
    "eval2_parscit = pd.read_pickle('/nlp/temp/eval2_parscit.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dummy_feats(parscit, maxlen):\n",
    "    parscit_feats = list(parscit['parscit_feat']) \n",
    "    parscit_padded = []\n",
    "    for ind in range(len(parscit_feats)):\n",
    "        parscit_mask = np.zeros((maxlen, 14), dtype='int8')\n",
    "        if len(parscit_feats[ind]) <= maxlen:\n",
    "            parscit_mask[0:len(parscit_feats[ind]), :] = parscit_feats[ind][:]\n",
    "        else:\n",
    "            parscit_mask[:] = parscit_feats[ind][0:maxlen,:]\n",
    "\n",
    "        parscit_padded.append(parscit_mask)\n",
    "    parscit_arr = np.array([i.tolist() for i in parscit_padded])\n",
    "    return parscit_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bla1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
